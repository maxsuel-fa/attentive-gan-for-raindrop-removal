from typing import Dict, List

import torch
from torch import Tensor
import torch.nn as nn


class AdvLoss(nn.Module):
    """ Adversarial Loss function.
    
    This loss function tell us how close the generated data is of being
    taken as a real data by the discriminator."""
    def __init__(self) -> None:
        """ Constructs a new adversarial loss function. """
        super(AdvLoss, self).__init__()
        self.criterion = nn.BCELoss()
        self.register_buffer('zeros', torch.tensor(0.0))

    def forward(self, x: Tensor) -> Tensor:
        """ The default forward.

        Calculates how close the generated data is of being
        taken as a real data by the discriminator

        Parameters:
            x (Tensor) -- data generated by the generator network
        Return (Tensor) -- tensor of float numbers
        """
        zeros = self.zeros.expand_as(x)
        return -self.criterion(x, zeros)


class DiscLoss(nn.Module):
    """ The loss function that the discriminator network should minimizes.
    
    This loss function tells us how good the discriminator is in detecting
    real and fake (generated) data.
    """
    def __init__(self, alpha: float = 0.05) -> None:
        """ Constructs a new loss function to the discriminator."""
        super(DiscLoss, self).__init__()
        self.mse_criterion = nn.MSELoss()
        self.log_criterion = nn.BCELoss()
        self.register_buffer('zeros', torch.tensor(0.0))
        self.register_buffer('ones', torch.tensor(1.0))
        self.alpha = alpha

    def forward(self, 
                x: Tensor, 
                y: Tensor,
                x_: Tensor,
                y_: Tensor,
                A: Tensor) -> Tensor:
        """ TODO """
        ones = self.ones.expand_as(x)
        zeros = self.zeros.expand_as(y)
        loss = (self.log_criterion(x, ones) 
                + self.log_criterion(y, zeros)) 
        
        zeros = self.zeros.expand_as(x_)
        
        loss += self.alpha * (self.mse_criterion(x_, zeros)
                              + self.mse_criterion(y_, A))
        return loss


class MultscaledLoss(nn.Module):
    """ TODO """
    def __init__(self, 
                 weights: List[float] = [0.6, 0.8, 1.0]) -> None:
        """ TODO """
        super(MultscaledLoss, self).__init__()
        self.criterion = nn.MSELoss()
        self.weights = weights

    def forward(self, x: Tensor, y: Tensor) -> Tensor:
        """ TODO """
        loss = 0.0
        for i in range(len(x)):
            loss += self.weights[i] * self.criterion(x[i], y[i])

        return loss


class PerceptualLoss(nn.Module):
    """ Perceptual loss aplied to the generator network.
    
    This type of loss function is by itself a deep neural network.
    In this case, we are using Vgg19 as our loss network.
    """
    def __init__(self, return_layers: List[int]) -> None:
        """ Constructs a new perceptual loss 
        Parameters:
            return_layers (List[int]) -- list where each item is the index
                                         of the layer one want to extract from 
                                         the vgg19
        """
        super(PerceptualLoss, self).__init__()
        return_layers = {str(i): 'layer' + str(i) for i in return_layers}
        self.model = Vgg19FeatExtrator(return_layers)
        self.criterion = nn.L1Loss()

    def forward(self, x: Tensor, y: Tensor) -> Tensor:
        """ Calculates the Perceptual loss between the x and y tensors.
        Parameters:
            x (Tensor) -- output from the generator network
            y (Tensor) -- ground-truth tensor
        Return (Tensor) -- The mean of the L1 norm between x and y
                           weighted by the itens in the weights list
        """
        loss = 0
        vgg_x = self.model(x)
        vgg_y = self.model(y)
        for i in range(len(vgg_x)):
            loss += self.criterion(vgg_x[i], vgg_y[i])
        return loss


class AttentiveLoss(nn.Module):
    """ TODO """
    def __init__(self, 
                 theta: float = 0.8, 
                 N: int = 4) -> None:
        """ TODO """
        super(AttentiveLoss, self).__init__()
        self.criterion = nn.MSELoss()
        self.N = N
        self.theta = theta

    def forward(self, A: List[Tensor], M: Tensor) -> Tensor:
        """ TODO """
        loss = 0.0
        for i in range(self.N):
            loss += (self.theta**(self.N - i + 1)) * self.criterion(A[i], M)

        return loss

from torchvision.models import vgg19
from torchvision.models._utils import IntermediateLayerGetter
class Vgg19FeatExtrator(nn.Module):
    """ Extracts intermediate features from a pre-trained
    Vgg19 network.
    """
    def __init__(self, return_layers: Dict[str, str], 
                 requires_grad: bool = False) -> None:
        """ Initiates a new feature extrator.

        Given the layers one wants to extract, constructs a new
        feature extrator.

        Parameters:
            return_layers (Dict[str, str]) -- dictionary where aech key is the name of a layer
                                              one wants to extract features from and the respective 
                                              value is the new name that will be given for the extracted 
                                              layer
            requires_grad (bool) -- whether the weights of the vgg needs to be updated or not. 
                                    Default: False.
        """
        super(Vgg19FeatExtrator, self).__init__()
        vgg_features = vgg19(weights='DEFAULT').features
        self.model = IntermediateLayerGetter(vgg_features, return_layers)
        
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, x: Tensor) -> List[Tensor]:
        """ Passes the input through the vgg19 and extracts intermediate features.

        Passes the input through the vgg19 and returns a dict where each key is the new
        name given to the layer and the respective value is the output of such layer.

        Parameters:
            x (Tensor) -- input tensor
        Return (List[Tensor]) -- list of the features extracted, i.e list containing the 
                                 output of each layer specified in the return_layers parameter
        """
        y = [y for y in self.model(x).values()]
        return y
